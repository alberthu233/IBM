{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://vision.skills.network/logo-light.png\" width=\"400\" alt=\"CV Studio logo\"  />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Transfer Learning with Convolutional Neural Networks For Classification with PyTorch and   <a href=\"https://vision.skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-cvstudio-2021-01-01\"> Computer Vision Learning \n",
    "Studio\n",
    " (CV Studio)</a></h2> <p><b> V 0.2</b></p>\n",
    "<h4>Project: Final_project_stop_signs</h4>\n",
    "<h4>Training Run: Transfer learning for Stop Sign Classification</h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **40** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will train a deep neural network for  image classification using <a href=\"https://cs231n.github.io/transfer-learning/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-cvstudio-2021-01-01\">transfer learning</a>, the image dataset will automatically be download from your <a href=\"https://vision.skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-cvstudio-2021-01-01\">CV Studio</a> account. Experiment with different hyperparameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you will train a state of the art image classifier using and <a href=\"https://vision.skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-cvstudio-2021-01-01\">CV Studio</a>, CV Studio is a fast, easy and collaborative open source image annotation tool for teams and individuals. In practice, very few people train an entire Convolutional Network from scratch (with random initialization), because it is relatively rare to have a dataset of sufficient size. Instead, it is common to pretrain a ConvNet on a very large dataset in the lab, then use this Network to train your model. We will use the Convolutional Network as a feature generator, only training the output layer.  In general, 100-200 images will give you a good starting point, and it only takes about half an hour.  Usually, the more images you add, the better your results, but it takes longer and the rate of improvement will decrease.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Import Libraries and Define Auxiliary Functions\n",
    "*   Create Dataset Object\n",
    "*   Load Model and Train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Define Auxiliary Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! conda install -c pytorch torchvision\n",
    "#! pip install skillsnetwork tqdm\n",
    "#!pip install  skillsnetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries for OS and Cloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import shutil\n",
    "import json\n",
    "import copy\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries for Data Processing and Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from matplotlib.pyplot import imshow\n",
    "from tqdm import tqdm\n",
    "from ipywidgets import IntProgress\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x21691699d50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader,random_split\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot train cost and validation accuracy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stuff(COST,ACC):    \n",
    "    fig, ax1 = plt.subplots()\n",
    "    color = 'tab:red'\n",
    "    ax1.plot(COST, color = color)\n",
    "    ax1.set_xlabel('Iteration', color = color)\n",
    "    ax1.set_ylabel('total loss', color = color)\n",
    "    ax1.tick_params(axis = 'y', color = color)\n",
    "    \n",
    "    ax2 = ax1.twinx()  \n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('accuracy', color = color)  # we already handled the x-label with ax1\n",
    "    ax2.plot(ACC, color = color)\n",
    "    ax2.tick_params(axis = 'y', color = color)\n",
    "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the transformed image:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow_(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp .permute(1, 2, 0).numpy() \n",
    "    print(inp.shape)\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the prediction and actual value:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result(model,x,y):\n",
    "    #x,y=sample\n",
    "    z=model(x.unsqueeze_(0))\n",
    "    _,yhat=torch.max(z.data, 1)\n",
    "    \n",
    "    if yhat.item()!=y:\n",
    "        text=\"predicted: {} actual: {}\".format(str(yhat.item()),y)\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our device as the first visible cuda device if we have CUDA available:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the device type is cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"the device type is\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will preprocess our dataset by changing the shape of the image, converting to tensor and normalizing the image channels. These are the default preprocessing steps for image data. In addition, we will perform data augmentation on the training dataset. The preprocessing steps for the test dataset is the same, but W do not prform data augmentation on the test dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>\n",
    "<p>mean = [0.485, 0.456, 0.406]</p>\n",
    "<p>std = [0.229, 0.224, 0.225]</p>\n",
    "<p>composed = transforms.Compose([transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),transforms.RandomRotation(degrees=5)\n",
    "                               , transforms.ToTensor()\n",
    "                               , transforms.Normalize(mean, std)])</p>\n",
    "    </code>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image from images folder and resize it to 224x224\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot some of our dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "\n",
    "\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "composed = transforms.Compose([transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),transforms.RandomRotation(degrees=5)\n",
    "                               , transforms.ToTensor()\n",
    "                               , transforms.Normalize(mean, std)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageFolder(root='images', transform=composed)\n",
    "# create train and val dataset\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_set, val_set = random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with different hyperparameters:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Epoch</b> indicates the number of passes of the entire training dataset, here we will set the number of epochs to 10:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Batch size</b> is the number of training samples utilized in one iteration. If the batch size is equal to the total number of samples in the training set, then every epoch has one iteration. In Stochastic Gradient Descent, the batch size is set to one. A batch size of 32--512 data points seems like a good value, for more information check out the following <a href=\"https://arxiv.org/abs/1609.04836?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-cvstudio-2021-01-01\">link</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Learning rate</b> is used in the training of neural networks. Learning rate is a hyperparameter with a small positive value, often in the range between 0.0 and 1.0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.000001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Momentum</b> is a term used in the gradient descent algorithm to improve training results:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "momentum=0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you set to <code>lr_scheduler=True</code>  for every epoch use a learning rate scheduler changes the range of the learning rate from a maximum or minimum value. The learning rate usually decays over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler=True\n",
    "base_lr=0.001\n",
    "max_lr=0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model and Train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader,validation_loader, criterion, optimizer, n_epochs,print_=True):\n",
    "    loss_list = []\n",
    "    accuracy_list = []\n",
    "    correct = 0\n",
    "    #global:val_set\n",
    "    n_test = len(val_set)\n",
    "    accuracy_best=0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    # Loop through epochs\n",
    "        # Loop through the data in loader\n",
    "    print(\"The first epoch should take several minutes\")\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        \n",
    "        loss_sublist = []\n",
    "        # Loop through the data in loader\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x, y=x.to(device), y.to(device)\n",
    "            model.train() \n",
    "\n",
    "            z = model(x)\n",
    "            loss = criterion(z, y)\n",
    "            loss_sublist.append(loss.data.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "        print(\"epoch {} done\".format(epoch) )\n",
    "\n",
    "        scheduler.step()    \n",
    "        loss_list.append(np.mean(loss_sublist))\n",
    "        correct = 0\n",
    "\n",
    "\n",
    "        for x_test, y_test in validation_loader:\n",
    "            x_test, y_test=x_test.to(device), y_test.to(device)\n",
    "            model.eval()\n",
    "            z = model(x_test)\n",
    "            _, yhat = torch.max(z.data, 1)\n",
    "            correct += (yhat == y_test).sum().item()\n",
    "        accuracy = correct / n_test\n",
    "        accuracy_list.append(accuracy)\n",
    "        if accuracy>accuracy_best:\n",
    "            accuracy_best=accuracy\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "        \n",
    "        if print_:\n",
    "            print('learning rate',optimizer.param_groups[0]['lr'])\n",
    "            print(\"The validaion  Cost for each epoch \" + str(epoch + 1) + \": \" + str(np.mean(loss_sublist)))\n",
    "            print(\"The validation accuracy for epoch \" + str(epoch + 1) + \": \" + str(accuracy)) \n",
    "    model.load_state_dict(best_model_wts)    \n",
    "    return accuracy_list,loss_list, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pre-trained model resnet18. Set the parameter pretrained to true.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet34(weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only train the last layer of the network set the parameter <code>requires_grad</code> to <code>False</code>, the network is a fixed feature extractor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "       param.requires_grad = False\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the output layer model.fc of the neural network with a nn.Linear object, to classify <code>n_classes</code> different classes. For the parameters in_features  remember the last hidden layer has 512 neurons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here\n",
    "model.fc = nn.Linear(512, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set device type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-entropy loss, or log loss, measures the performance of a classification model combines LogSoftmax in one object class. It is useful when training a classification problem with C classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a training loader and validation loader object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_set , batch_size=batch_size,shuffle=True)\n",
    "validation_loader= torch.utils.data.DataLoader(dataset=val_set , batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the optim package to define an Optimizer that will update the weights of the model for us.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use <a href='https://arxiv.org/pdf/1506.01186.pdf?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-cvstudio-2021-01-01'>Cyclical Learning Rates</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lr_scheduler:\n",
    "    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=0.01,step_size_up=5,mode=\"triangular2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to train model,for 500 images this take 25 minutes, depending on your dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-Coursera/images/this_make_take_time.gif\" alt=\"this may take some time\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first epoch should take several minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:05<01:42,  5.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate 0.002800000000000002\n",
      "The validaion  Cost for each epoch 1: 0.7096982300281525\n",
      "The validation accuracy for epoch 1: 0.65\n",
      "epoch 1 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:10<01:38,  5.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate 0.0046\n",
      "The validaion  Cost for each epoch 2: 0.5947705606619517\n",
      "The validation accuracy for epoch 2: 0.85\n",
      "epoch 2 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:16<01:30,  5.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate 0.006400000000000001\n",
      "The validaion  Cost for each epoch 3: 0.43756259481112164\n",
      "The validation accuracy for epoch 3: 0.75\n",
      "epoch 3 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:21<01:25,  5.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate 0.008199999999999999\n",
      "The validaion  Cost for each epoch 4: 0.4404870569705963\n",
      "The validation accuracy for epoch 4: 0.8\n",
      "epoch 4 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [00:26<01:19,  5.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate 0.010000000000000002\n",
      "The validaion  Cost for each epoch 5: 0.2815154244502385\n",
      "The validation accuracy for epoch 5: 0.75\n",
      "epoch 5 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [00:31<01:13,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate 0.008199999999999999\n",
      "The validaion  Cost for each epoch 6: 0.2986902768413226\n",
      "The validation accuracy for epoch 6: 0.7\n",
      "epoch 6 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [00:36<01:07,  5.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate 0.006400000000000001\n",
      "The validaion  Cost for each epoch 7: 0.17500633373856544\n",
      "The validation accuracy for epoch 7: 0.9\n",
      "epoch 7 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [00:42<01:02,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate 0.0046\n",
      "The validaion  Cost for each epoch 8: 0.21918929740786552\n",
      "The validation accuracy for epoch 8: 0.85\n",
      "epoch 8 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [00:47<00:57,  5.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate 0.002800000000000002\n",
      "The validaion  Cost for each epoch 9: 0.16847712049881616\n",
      "The validation accuracy for epoch 9: 0.85\n",
      "epoch 9 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [00:52<00:52,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate 0.001\n",
      "The validaion  Cost for each epoch 10: 0.1840218131740888\n",
      "The validation accuracy for epoch 10: 0.95\n",
      "epoch 10 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [00:58<00:47,  5.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate 0.001900000000000001\n",
      "The validaion  Cost for each epoch 11: 0.14877142757177353\n",
      "The validation accuracy for epoch 11: 0.75\n",
      "epoch 11 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [01:03<00:42,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate 0.002800000000000002\n",
      "The validaion  Cost for each epoch 12: 0.26399797449509305\n",
      "The validation accuracy for epoch 12: 0.85\n",
      "epoch 12 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [01:08<00:37,  5.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate 0.003699999999999999\n",
      "The validaion  Cost for each epoch 13: 0.21019071464737257\n",
      "The validation accuracy for epoch 13: 0.95\n",
      "epoch 13 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [01:13<00:31,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate 0.0046\n",
      "The validaion  Cost for each epoch 14: 0.23845400661230087\n",
      "The validation accuracy for epoch 14: 0.85\n",
      "epoch 14 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [01:19<00:26,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate 0.0055000000000000005\n",
      "The validaion  Cost for each epoch 15: 0.1879103183746338\n",
      "The validation accuracy for epoch 15: 0.8\n",
      "epoch 15 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [01:25<00:22,  5.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate 0.0046\n",
      "The validaion  Cost for each epoch 16: 0.1626851943631967\n",
      "The validation accuracy for epoch 16: 0.95\n",
      "epoch 16 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [01:30<00:16,  5.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate 0.003699999999999999\n",
      "The validaion  Cost for each epoch 17: 0.13979708527525267\n",
      "The validation accuracy for epoch 17: 0.95\n",
      "epoch 17 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [01:36<00:11,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate 0.002800000000000002\n",
      "The validaion  Cost for each epoch 18: 0.12892030676205954\n",
      "The validation accuracy for epoch 18: 0.8\n",
      "epoch 18 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [01:42<00:05,  5.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate 0.001900000000000001\n",
      "The validaion  Cost for each epoch 19: 0.1791679486632347\n",
      "The validation accuracy for epoch 19: 0.9\n",
      "epoch 19 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:47<00:00,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate 0.001\n",
      "The validaion  Cost for each epoch 20: 0.12389399421711762\n",
      "The validation accuracy for epoch 20: 0.95\n",
      "elapsed time 107.64263033866882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_datetime = datetime.now()\n",
    "start_time=time.time()\n",
    "\n",
    "accuracy_list,loss_list, model=train_model(model,train_loader , validation_loader, criterion, optimizer, n_epochs=n_epochs)\n",
    "\n",
    "end_datetime = datetime.now()\n",
    "current_time = time.time()\n",
    "elapsed_time = current_time - start_time\n",
    "print(\"elapsed time\", elapsed_time )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the following to report back the results of the training run to CV Studio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model to model.pt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to model.pt\n",
    "torch.save(model.state_dict(), 'model_1.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot train cost and validation accuracy,  you can improve results by getting more data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_stuff(loss_list,accuracy_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model that performs best:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.resnet34(pretrained=True)\n",
    "model.fc = nn.Linear(512, 2)\n",
    "model.load_state_dict(torch.load( \"model_1.pt\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also deploy your model via Web Application or Web App . This allows users to interact with your model like a website. They can upload the image with a user interface and view the results. Let's see how we can deploy a web app in CV Studio. In CV Studio, go to the use model section and select New Application. Fill out the window as follows,  giving  your model a name and selecting the Model in this project, select **TEST - 1-click Deploy your Model to Cloud (Code Engine)** and  select the model from the  training run as shown here:\n",
    "\n",
    "<p>\n",
    "<img  src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images/Image_2021-05-20_at_8.04_AM.jpeg\"  alt=\"popup\" width=\"400\" height=\"500\">\n",
    "</p>\n",
    "\n",
    "Then once the window is filled out press the Create Application button and your model will begin deploying.\n",
    "\n",
    "<p>\n",
    "<img  src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images/Image_2021-05-20_at_8.07_AM.jpeg\"  alt=\"popup\" width=\"500\" height=\"100\">\n",
    "</p>\n",
    "\n",
    "Wait until the status changes from \"deploying\" to \"ready\". Once the status changes to ready, your application is ready for you to use!\n",
    "\n",
    "<p>\n",
    "<img  src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images/Image_2021-05-20_at_8.08_AM.jpeg\"  alt=\"popup\" width=\"500\" height=\"100\">\n",
    "</p>\n",
    "\n",
    "You can press the URL to go to your web application.\n",
    "\n",
    "<p>\n",
    "<img  src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images/Image_2021-05-20_at_3.12_PM.jpeg\"  alt=\"popup\" width=\"500\" height=\"400\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joseph Santarcangelo,has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Date (YYYY-MM-DD) | Version | Changed By | Change Description      |\n",
    "| ----------------- | ------- | ---------- | ----------------------- |\n",
    "| 2021-05-25        | 0.3     | Yasmine    | Modifies Multiple Areas |\n",
    "| 2021-05-25        | 0.3     | Kathy      | Modified Multple Areas. |\n",
    "| 2021-03-08        | 0.2     | Joseph     | Modified Multiple Areas |\n",
    "| 2021-02-01        | 0.1     | Joseph     | Modified Multiple Areas |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2021 IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('tensorflow-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e2f6deba557dc7d8d1b4cf161216c8920ae0cc2d6a027649b4e5d7481f4460e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
